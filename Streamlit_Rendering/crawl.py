# Streamlit_Rendering/crawl.py
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import time
import json

# ============================================================
# ì„¤ì •
# ============================================================
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Referer": "https://news.naver.com/"
}

NEWS_CATEGORY = {
    "ì •ì¹˜": 100, "ê²½ì œ": 101, "ì‚¬íšŒ": 102,
    "ìƒí™œ/ë¬¸í™”": 103, "ì„¸ê³„": 104, "IT/ê³¼í•™": 105,
    "ì—°ì˜ˆ": 106, "ìŠ¤í¬ì¸ ": 107
}

MAX_WORKERS = 10
START_DATE = datetime.now() - timedelta(days=7)
END_DATE = datetime.now() + timedelta(days=1)

MEDIA_KEYWORDS = [
    'ì‹ ë¬¸', 'ë‰´ìŠ¤', 'ë°©ì†¡', 'ì¼ë³´', 'ì—°í•©', 'ë‰´ì‹œìŠ¤', 'ë‰´ìŠ¤1',
    'ê²½ì œ', 'TV', 'ë°ì¼ë¦¬', 'ë¯¸ë””ì–´', 'í¬í† ', 'ê¸°ìë‹¨',
    'í—¤ëŸ´ë“œ', 'íƒ€ì„ì¦ˆ'
]


# ============================================================
# ìœ í‹¸ í•¨ìˆ˜
# ============================================================
def _clean_text(s: str) -> str:
    """
    í…ìŠ¤íŠ¸ ê°„ë‹¨ ì •ì œ (ê³µë°± í†µí•©)
    """
    s = re.sub(r'\s+', ' ', s or '').strip()
    return s


def clean_news_content(text):
    """ë³¸ë¬¸ ë‚´ìš© ì •ì œ (ìƒì„¸ ë²„ì „)"""
    if not text:
        return ""
    text = re.sub(r'^[ê°€-í£]+(?:ì¼ë³´|ì‹ ë¬¸)\s*', '', text)
    text = re.sub(r'\s*[ê°€-í£]+(?:ì¼ë³´|ì‹ ë¬¸)$', '', text)
    text = re.sub(r'(?:ì´?ë©”ì¼|email|ì¹´í†¡|ì¹´ì¹´ì˜¤í†¡|ì•„ì´ë””|ID)\s*[:]?\s*[a-zA-Z0-9._%+-]+(@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}|[a-zA-Z0-9_]+)', '', text, flags=re.I)
    text = re.sub(r'[â˜â€»].*?(?=\s{2,}|$)', '', text)
    text = re.sub(r'ë¶€ê³ \s*ê²Œì¬\s*ë¬¸ì˜.*?([0-9\-\s]+|íŒ©ìŠ¤|ì „í™”|ì´ë©”ì¼|ì¹´í†¡|okjebo)+', '', text)
    text = re.sub(r'(?:ì „í™”|ì—°ë½ì²˜|ë¬¸ì˜|íŒ©ìŠ¤|HP|TEL)\s*[:]?\s*\d{2,3}[-\s]\d{3,4}[-\s]\d{4}', '', text)
    text = re.sub(r'â–³[ê°€-í£]{2,4}\s*=\s*.*?(?=\s*â–³|\s*$)', '', text)
    text = re.sub(r'(?:íŠ¸ìœ„í„°|í˜ì´ìŠ¤ë¶)\s+[@\w\./\-_]+', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
    text = re.sub(r'\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}|\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼', '', text)
    text = re.sub(r'<.*?>|ã€.*?ã€‘|\[.*?\]|\(.*?\)', '', text)
    text = re.sub(r'[â—|]\s*[ê°€-í£]{2,4}\s*ê¸°ì', '', text)
    text = re.sub(r'[ê°€-í£]{2,4}\s*ê¸°ì\s*[=|-]\s*', '', text)
    text = re.sub(r'[â—â–²â–¶â“’â’¸â– â”â—†â–¶â—€â–¨â–£â—ˆ*Â·]', '', text)
    return _clean_text(text)  # ìµœì¢… ê³µë°± ì •ì œ


def parse_date(text):
    """ë‚ ì§œ íŒŒì‹±"""
    if not text:
        return None
    try:
        if "-" in text:
            return datetime.strptime(text[:16], "%Y-%m-%d %H:%M")
        clean = re.sub(r'[^0-9.: ]', '', text)
        is_pm = "ì˜¤í›„" in text
        dt = datetime.strptime(clean.strip(), "%Y.%m.%d. %H:%M")
        if is_pm and dt.hour != 12:
            dt += timedelta(hours=12)
        return dt
    except:
        return None


def get_naver_stats(url):
    """ë„¤ì´ë²„ ëŒ“ê¸€ ìˆ˜, ì¶”ì²œ ìˆ˜ ì¡°íšŒ"""
    stats = {"comment_cnt": 0, "like_cnt": 0}
    try:
        m = re.search(r'article/(\d+)/(\d+)', url)
        if not m:
            return stats
        oid, aid = m.group(1), m.group(2)
        
        # ì¶”ì²œ ìˆ˜
        like_url = f"https://news.like.naver.com/v1/search/contents?q=NEWS[ne_{oid}_{aid}]"
        like_res = requests.get(like_url, headers=DEFAULT_HEADERS, timeout=5)
        if like_res.status_code == 200:
            stats["like_cnt"] = sum(
                r.get("count", 0) for r in like_res.json()["contents"][0].get("reactions", [])
            )
        
        # ëŒ“ê¸€ ìˆ˜
        comment_url = f"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&pool=cbox5&lang=ko&objectId=news{oid},{aid}"
        c_res = requests.get(comment_url, headers={"Referer": url}, timeout=5)
        jm = re.search(r'\((.*)\)', c_res.text)
        if jm:
            data = json.loads(jm.group(1))
            if data.get("success"):
                stats["comment_cnt"] = data["result"]["count"]["comment"]
    except:
        pass
    return stats


def clean_reporter_name(name):
    """ê¸°ìëª… ì •ì œ"""
    if not name:
        return "ë¯¸ìƒ"
    name = re.sub(r'\(.*?\)', '', name).replace("ê¸°ì", "").strip()
    return name.split()[0] if name.split() else "ë¯¸ìƒ"


def compute_jaccard_similarity(t1, t2):
    """Jaccard ìœ ì‚¬ë„ ê³„ì‚°"""
    s1 = set(re.findall(r'\w+', t1))
    s2 = set(re.findall(r'\w+', t2))
    return len(s1 & s2) / len(s1 | s2) if s1 | s2 else 0


def remove_similar_articles(articles, threshold=0.7):
    """ìœ ì‚¬í•œ ê¸°ì‚¬ ì œê±°"""
    # í‚¤ê°’ ë³€ê²½ ë°˜ì˜: full_text -> content
    sorted_articles = sorted(articles, key=lambda x: len(x["content"]), reverse=True)
    final, dropped = [], set()
    for i in range(len(sorted_articles)):
        if i in dropped:
            continue
        base = sorted_articles[i]
        final.append(base)
        for j in range(i + 1, len(sorted_articles)):
            if j not in dropped and compute_jaccard_similarity(base["title"], sorted_articles[j]["title"]) >= threshold:
                dropped.add(j)
    return final


# ============================================================
# ë§í¬ ìˆ˜ì§‘ ë¡œì§
# ============================================================
def collect_and_convert_links(url, category, max_links=200):
    """ì—”í„°í…Œì¸ë¨¼íŠ¸/ìŠ¤í¬ì¸  ë§í¬ ìˆ˜ì§‘"""
    links = []
    try:
        res = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        for a in soup.select("a[href*='article']"):
            href = a.get("href", "")
            m = re.search(r'article/(\d+)/(\d+)', href) or re.search(r'oid=(\d+)&aid=(\d+)', href)
            if m:
                s_url = f"https://n.news.naver.com/article/{m.group(1)}/{m.group(2)}"
                if (s_url, category) not in links:
                    links.append((s_url, category))
            if len(links) >= max_links:
                break
    except:
        pass
    return links


def collect_article_urls(sid, category, max_links=200):
    """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ URL ìˆ˜ì§‘"""
    links, page = [], 1
    date = datetime.now().strftime("%Y%m%d")
    while len(links) < max_links:
        url = f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid}&date={date}&page={page}"
        try:
            res = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
            soup = BeautifulSoup(res.text, "html.parser")
            found = soup.select(".type06_headline a, .type06 a")
            if not found:
                break
            for a in found:
                m = re.search(r'article/(\d+)/(\d+)', a.get("href", ""))
                if m:
                    s_url = f"https://n.news.naver.com/article/{m.group(1)}/{m.group(2)}"
                    if (s_url, category) not in links:
                        links.append((s_url, category))
                if len(links) >= max_links:
                    break
            page += 1
            time.sleep(0.1)
        except:
            break
    return links


# ============================================================
# ê¸°ì‚¬ í¬ë¡¤ë§ (ë³¸ë¬¸ 200ì í•„í„°ë§) - ìˆ˜ì •ë¨
# ============================================================
def crawl_article(url_cat):
    """
    ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§ (ì–¸ë¡ ì‚¬ ì¶”ì¶œ ë¡œì§ ê°•í™” ë° ë°˜í™˜ í¬ë§· ìˆ˜ì •)
    """
    url, category = url_cat
    try:
        res = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        
        # ì–¸ë¡ ì‚¬(source) ì¶”ì¶œ ë¡œì§
        source_tag = soup.select_one("em.media_end_head_top_logo img")
        source = source_tag.get("alt") if source_tag else "ë¯¸ìƒ"
        if source == "ë¯¸ìƒ": # ë°±ì—…ìš© ì¶”ì¶œ ë°©ì‹
            source_meta = soup.select_one('meta[property="og:article:author"]')
            source = source_meta.get("content").split("|")[0].strip() if source_meta else "ë¯¸ìƒ"

        # ì œëª©
        title_tag = soup.select_one("h2#title_area, .media_end_head_headline")
        title = re.sub(r'\[.*?\]', '', title_tag.get_text(strip=True)) if title_tag else None
        if not title or len(title) < 6:
            return None

        # ë‚ ì§œ
        date_tag = soup.select_one("span.media_end_head_info_datestamp_time")
        dt = parse_date(date_tag.get("data-date-time")) if date_tag else None
        if dt and not (START_DATE <= dt <= END_DATE):
            return None

        # ë³¸ë¬¸
        content_div = soup.select_one("div._article_content, #dic_area")
        if not content_div:
            return None
        for j in content_div.select("script, style, .ad_area, .img_desc"):
            j.decompose()
        content = content_div.get_text(" ", strip=True)

        # 200ì í•„í„°ë§
        if len(content) < 200:
            return None

        # ê¸°ìëª…
        rep_tag = soup.select_one("em.media_end_head_journalist_name, .byline_s")
        reporter = clean_reporter_name(rep_tag.get_text(strip=True) if rep_tag else "ë¯¸ìƒ")
        
        # ëŒ“ê¸€, ì¶”ì²œ ìˆ˜
        stats = get_naver_stats(url)

        # ìš”ì²­í•˜ì‹  ë°˜í™˜ í¬ë§·ìœ¼ë¡œ ìˆ˜ì •
        return {
            "date": dt.strftime("%Y-%m-%d %H:%M") if dt else "ë‚ ì§œë¯¸ìƒ",
            "category": category,
            "source": source,
            "title": title,
            "reporter": reporter,
            "comment_cnt": stats["comment_cnt"],
            "like_cnt": stats["like_cnt"],
            "link": url,
            "content": content
        }
    except:
        return None


# ============================================================
# ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜ (admin_pipelineì—ì„œ í˜¸ì¶œ)
# ============================================================
def fetch_articles_from_naver(max_articles_per_category=30):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ (ì‹¤ í¬ë¡¤ëŸ¬)
    """
    
    print("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
    all_links = []
    
    for cat, sid in NEWS_CATEGORY.items():
        if sid in [106, 107]:  # ì—°ì˜ˆ, ìŠ¤í¬ì¸ 
            url = f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1={sid}"
            all_links.extend(collect_and_convert_links(url, cat, max_articles_per_category))
        else:
            all_links.extend(collect_article_urls(sid, cat, max_articles_per_category))
        time.sleep(0.5)

    if not all_links:
        print("âŒ ìˆ˜ì§‘ëœ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    print(f"ğŸ“° {len(all_links)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ. í¬ë¡¤ë§ ì‹œì‘...")
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(crawl_article, link) for link in all_links]
        for f in tqdm(as_completed(futures), total=len(futures), desc="í¬ë¡¤ë§"):
            r = f.result()
            if r:
                results.append(r)

    if not results:
        print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    df = pd.DataFrame(results)
    print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")

    # ë³¸ë¬¸ ì •ì œ
    print("ğŸ§¹ ë³¸ë¬¸ ë…¸ì´ì¦ˆ ì œê±° ì¤‘...")
    df["content"] = df["content"].apply(clean_news_content)

    # ì¤‘ë³µ ì œê±°
    df['title'] = df['title'].str.replace(r'\(í’€ì˜ìƒ\)', '', regex=True)\
                              .str.replace(r'\[.*?\]', '', regex=True)\
                              .str.strip()
    df['tmp_title_len'] = df['title'].str.len()
    df = df.sort_values(by='tmp_title_len', ascending=False)\
            .drop_duplicates(subset=['content'], keep='first')\
            .drop(columns=['tmp_title_len'])

    # ê¸°ìëª… í•„í„°ë§
    df = df[df["reporter"] != "ë¯¸ìƒ"]
    df = df[~df["reporter"].str.contains(r'\?|[a-zA-Z]|' + "|".join(MEDIA_KEYWORDS), na=False)]
    df = df[(df["reporter"].str.len() >= 2) & (df["reporter"].str.len() < 5)]

    # ìœ ì‚¬í•œ ê¸°ì‚¬ ì œê±°
    final_data = remove_similar_articles(df.to_dict("records"), threshold=0.7)

    if final_data:
        df_final = pd.DataFrame(final_data)
        df_final["reporter"] = df_final["reporter"].apply(
            lambda x: x if "ê¸°ì" in x else f"{x} ê¸°ì"
        )
        print(f"âœ… ìµœì¢… {len(df_final)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return df_final
    else:
        print("âŒ í•„í„°ë§ í›„ ë‚¨ì€ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()


# ============================================================
# Backward compatibility (ê¸°ì¡´ fetch_article_from_url ìœ ì§€)
# ============================================================
def fetch_article_from_url(url: str, source: str = "manual", timeout_sec: int = 10) -> pd.DataFrame:
    """
    ë‹¨ì¼ URL í¬ë¡¤ë§ (í˜¸í™˜ì„± ìœ ì§€)
    """
    now = datetime.now().isoformat()

    try:
        res = requests.get(url, headers=DEFAULT_HEADERS, timeout=timeout_sec)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "lxml")

        title = _clean_text(
            re.sub(r'\[.*?\]', '', soup.title.get_text().strip())
            if soup.title
            else "Untitled"
        )

        raw_text = _clean_text(soup.get_text(" "))
        body_text = clean_news_content(raw_text)

        if len(body_text) > 4000:
            body_text = body_text[:4000] + "..."

        article_id = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"

        df = pd.DataFrame([{
            "article_id": article_id,
            "title": title,
            "source": source,
            "url": url,
            "published_at": now,
            "content": body_text, # í˜¸í™˜ì„±ì„ ìœ„í•´ í‚¤ê°’ contentë¡œ í†µì¼
        }])
        return df
    except Exception as e:
        print(f"âŒ URL í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
        return pd.DataFrame()

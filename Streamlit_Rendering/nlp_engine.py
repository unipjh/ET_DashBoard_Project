# íŒŒì¼ëª…: Streamlit_Rendering/nlp_engine.py

import torch
import numpy as np
import pandas as pd
import re
import warnings
import json
from sklearn.metrics.pairwise import cosine_similarity
from kobert_transformers import get_tokenizer, get_kobert_model
from collections import Counter
from tqdm import tqdm
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

# ê²½ê³  ë©”ì‹œì§€ ì°¨ë‹¨
warnings.filterwarnings("ignore", category=UserWarning, module='sklearn')

class FastKoBertSummarizer:
    def __init__(self):
        """ëª¨ë¸ ë¡œë“œ ë° ì´ˆê¸°í™”"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # 1. KoBERT (ìš”ì•½ ë° ì„ë² ë”©ìš©)
        try:
            self.tokenizer = get_tokenizer()
            self.model = get_kobert_model()
            self.model.to(self.device)
            self.model.eval()
            print("âœ… KoBERT model loaded successfully.")
        except Exception as e:
            print(f"âŒ KoBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None

        # 2. KeyBERT (í‚¤ì›Œë“œ ì¶”ì¶œìš©, GPU í• ë‹¹)
        print("â³ Loading KeyBERT model...")
        try:
            st_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=self.device)
            self.kw_model = KeyBERT(model=st_model)
            print(f"âœ… KeyBERT model loaded on {self.device}.")
        except Exception as e:
            print(f"âŒ KeyBERT ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.kw_model = None

        # ë¶ˆìš©ì–´ ì„¸íŠ¸ (Set)
        self.stopwords_set = {
            'ê¸°ì', 'íŠ¹íŒŒì›', 'ì•µì»¤', 'ë‰´ìŠ¤', 'ì—°í•©ë‰´ìŠ¤', 'í†µì‹ ', 'ì‹ ë¬¸', 'ë³´ë„', 'ì†ë³´', 'ë‹¨ë…',
            'ì¢…í•©', 'ì·¨ì¬', 'ì‚¬ì§„', 'ì˜ìƒ', 'ìº¡ì²˜', 'ì œê³µ', 'ìë£Œ', 'ì¶œì²˜', 'ê¸°ì‚¬', 'í¸ì§‘', 'ë°œí–‰',
            'ì €ì‘ê¶Œ', 'ë¬´ë‹¨ì „ì¬', 'ì¬ë°°í¬', 'ê¸ˆì§€', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì•Œë¦¼', 'ì œë³´', 'ë¬¸ì˜', 'í™ˆí˜ì´ì§€',
            'ì‚¬ì´íŠ¸', 'ë§í¬', 'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¸ìŠ¤íƒ€ê·¸ë¨', 'ìœ íŠœë¸Œ', 'ì±„ë„', 'ê²€ìƒ‰', 'ì¹´í†¡', 'ë¼ì¸',
            'ì•±', 'ì–´í”Œ', 'ë‹¤ìš´ë¡œë“œ', 'í´ë¦­', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ì „í™”', 'ì´ë©”ì¼', 'ë‰´ìŠ¤1', 'VJ', 'ì˜ìƒê¸°ì',
            'ì§€ë‚œ', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì´ë‚ ', 'ì „ë‚ ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'í˜„ì¬', 'ìµœê·¼', 'ë‹¹ì‹œ',
            'ì§í›„', 'ì´í›„', 'ì´ì „', 'ì•ì„œ', 'ì˜¤ì „', 'ì˜¤í›„', 'ìƒˆë²½', 'ë°¤', 'ë‚®', 'ì£¼ë§', 'í‰ì¼', 'ì—°íœ´',
            'ì‹œì‘', 'ì¢…ë£Œ', 'ì˜ˆì •', 'ê³„íš', 'ì§„í–‰', 'ê³¼ì •', 'ë‹¨ê³„', 'ì‹œì ', 'ì‹œê¸°', 'ê¸°ê°„', 'ë™ì•ˆ',
            'ë‚´ë…„', 'ì˜¬í•´', 'ì§€ë‚œí•´', 'ì‘ë…„', 'ë¶„ê¸°', 'ìƒë°˜ê¸°', 'í•˜ë°˜ê¸°', 'ê²°ê³¼',
            'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ì•Œë ¸ë‹¤', 'ë³´ì¸ë‹¤', 'ì„¤ëª…í–ˆë‹¤', 'ê°•ì¡°í–ˆë‹¤', 'ë§ë¶™ì˜€ë‹¤',
            'ì£¼ì¥í–ˆë‹¤', 'ë¹„íŒí–ˆë‹¤', 'ì§€ì í–ˆë‹¤', 'ì–¸ê¸‰í–ˆë‹¤', 'ë°œí‘œí–ˆë‹¤', 'ê³µê°œí–ˆë‹¤', 'í™•ì¸í–ˆë‹¤',
            'íŒŒì•…ëë‹¤', 'ì•Œë ¤ì¡Œë‹¤', 'ë‚˜íƒ€ë‚¬ë‹¤', 'ê¸°ë¡í–ˆë‹¤', 'í’€ì´ëœë‹¤', 'í•´ì„ëœë‹¤', 'ë¶„ì„ëœë‹¤',
            'ì „ë§ëœë‹¤', 'ì˜ˆìƒëœë‹¤', 'ê´€ì¸¡ëœë‹¤', 'ë³´ë„í–ˆë‹¤', 'ì¸ìš©í–ˆë‹¤', 'ì œì•ˆí–ˆë‹¤', 'ìš”ì²­í–ˆë‹¤',
            'ì´‰êµ¬í–ˆë‹¤', 'ì§€ì‹œí–ˆë‹¤', 'í•©ì˜í–ˆë‹¤', 'ê²°ì •í–ˆë‹¤', 'í™•ì •í–ˆë‹¤', 'ì¶”ì§„í–ˆë‹¤', 'ê²€í† í–ˆë‹¤',
            'ë…¼ì˜í–ˆë‹¤', 'í˜‘ì˜í–ˆë‹¤', 'ê°œìµœí–ˆë‹¤', 'ì°¸ì„í–ˆë‹¤', 'ë¶ˆì°¸í–ˆë‹¤', 'ëë‹¤', 'í–ˆë‹¤', 'ëœë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤',
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ê³³', 'ì¤‘', 'ë§Œ', 'ë¿', 'ë°', 'ë°”', 'ì¸¡', 'ë¶„', 'ê°œ', 'ëª…', 'ì›', 'ê±´',
            'ìœ„', 'ì ', 'ë©´', 'ì±„', 'ì‹', 'í¸', 'ë§Œí¼', 'ëŒ€ë¡œ', 'ê´€ë ¨', 'ëŒ€í•´', 'ëŒ€í•œ', 'ìœ„í•´', 'í†µí•´',
            'ë”°ë¼', 'ì˜í•´', 'ì¸í•´', 'ëŒ€ë¹„', 'ê¸°ì¤€', 'ì •ë„', 'ìˆ˜ì¤€', 'ê·œëª¨', 'ë¹„ì¤‘', 'ê°€ëŠ¥ì„±', 'í•„ìš”ì„±',
            'ì¤‘ìš”ì„±', 'ë¬¸ì œ', 'ë‚´ìš©', 'ë¶€ë¶„', 'ë¶„ì•¼', 'ì˜ì—­', 'ë²”ìœ„', 'ëŒ€ìƒ', 'ê´€ê³„', 'ì‚¬ì´', 'ìƒí™©',
            'ì—¬ê±´', 'ì¡°ê±´', 'ë¶„ìœ„ê¸°', 'íë¦„', 'ì¶”ì„¸', 'í˜„ìƒ', 'ì‹¤íƒœ', 'í˜„í™©', 'ëª¨ìŠµ', 'ì–‘ìƒ', 'í˜•íƒœ',
            'êµ¬ì¡°', 'ì²´ê³„', 'ì‹œìŠ¤í…œ', 'ë°©ì‹', 'ë°©ë²•', 'ìˆ˜ë‹¨', 'ê²°ê³¼', 'ì›ì¸', 'ì´ìœ ', 'ë°°ê²½', 'ëª©ì ',
            'ëª©í‘œ', 'ì˜ë„', 'ì·¨ì§€', 'ì˜ë¯¸', 'ì—­í• ', 'ê¸°ëŠ¥', 'íš¨ê³¼', 'ì˜í–¥', 'ê°€ì¹˜', 'ìì‹ ', 'ìƒê°', 'ì‚¬ëŒ',
            'ë°', 'ë˜', 'ë˜ëŠ”', 'í˜¹ì€', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë°˜ë©´', 'í•œí¸', 'ê²Œë‹¤ê°€',
            'ì•„ìš¸ëŸ¬', 'ë”ë¶ˆì–´', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ê·¸ë˜ì„œ', 'ê²°êµ­', 'ì¦‰', 'ê³§', 'ë‹¤ì‹œ',
            'íŠ¹íˆ', 'ë¬´ì—‡ë³´ë‹¤', 'ë¬¼ë¡ ', 'ì‹¤ì œë¡œ', 'ì‚¬ì‹¤', 'ëŒ€ì²´ë¡œ', 'ì¼ë°˜ì ìœ¼ë¡œ', 'ì£¼ë¡œ',
            'ê°€ë”', 'ìì£¼', 'í•­ìƒ', 'ì´ë¯¸', 'ë²Œì¨', 'ì•„ì§', 'ì´ì œ', 'ì§€ê¸ˆ', 'ë‹¹ì¥', 'ì ì°¨',
            'ì ì ', 'ê°ˆìˆ˜ë¡', 'ë”ìš±', 'í›¨ì”¬', 'ë§¤ìš°', 'ì•„ì£¼', 'ë„ˆë¬´', 'ìƒë‹¹íˆ', 'ë‹¤ì†Œ', 'ì˜ìƒí¸ì§‘', 'ì˜ìƒì·¨ì¬',
            'ì•½ê°„', 'ì „í˜€', 'ë°˜ë“œì‹œ', 'ì˜¤ì§', 'ë‹¤ë§Œ', 'ë‹¨ì§€', 'ì˜¤ë¡œì§€', 'ë§ˆì¹˜', 'ê²°êµ­ì€',
            'ê²½ìš°', 'ë•Œë¬¸', 'ê°€ì¥', 'ìì²´', 'ì£¼ìš”', 'ê°ê°', 'ë˜í•œ', 'ë‹¬ë¼', 'ì—­ì‹œ', 'ëª¨ë‘', 'ë°”ë¡œ', 'ê²ƒìœ¼ë¡œ', 'í•˜ëŠ”', 'ìˆëŠ”'
        }
        self.stopwords_list = list({w.lower() for w in self.stopwords_set})

    def _preprocess_text(self, text):
        if not text: return ""
        text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '', text)
        text = re.sub(r'\[.*?\]|\(.*?\)|\<.*?\>|â—€.*?â–¶', '', text)
        text = re.sub(r'@[a-zA-Z0-9ê°€-í£_]+', '', text)
        text = re.sub(r'\w{2,4} ê¸°ì', '', text)

        remove_words = ['ë‰´ìŠ¤1', 'ì—°í•©ë‰´ìŠ¤', 'ë‰´ì‹œìŠ¤', 'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'KBS', 'SBS', 'MBC', 'VJ', 'ì˜ìƒê¸°ì', 'ì¹´í†¡', 'ë¼ì¸', 'í™ˆí˜ì´ì§€', 'ì˜ìƒí¸ì§‘', 'ì˜ìƒì·¨ì¬',
                        'ì˜ìƒ', 'ìº¡ì²˜', 'ì œê³µ', 'ìë£Œ', 'ì¶œì²˜', 'ê¸°ì‚¬', 'í¸ì§‘', 'ë°œí–‰', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì•Œë¦¼', 'ì‚¬ì´íŠ¸', 'ë§í¬', ' .',
                        'ê¸°ì', 'íŠ¹íŒŒì›', 'ì•µì»¤', 'ë‹¤ìš´ë¡œë“œ', 'í´ë¦­', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¸ìŠ¤íƒ€ê·¸ë¨', 'ìœ íŠœë¸Œ', 'ì±„ë„']
        for word in remove_words:
            text = text.replace(word, '')
        text = re.sub(r'[^ \.\,\?\!\wê°€-í£]', '', text)
        return text.strip()

    def get_embedding_batch(self, texts, max_length=128, batch_size=32):
        if not texts or self.model is None:
            return np.zeros((len(texts), 768))

        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i : i + batch_size]
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**inputs)

            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            all_embeddings.append(cls_embeddings)

        if not all_embeddings:
            return np.zeros((0, 768))
        return np.vstack(all_embeddings)

    def _extract_keywords(self, text, top_n=5):
        if not text or self.kw_model is None:
            return []
        try:
            keywords_tuples = self.kw_model.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 1),
                stop_words=self.stopwords_list,
                top_n=top_n,
                use_mmr=True,
                diversity=0.3
            )
            return [kw[0] for kw in keywords_tuples]
        except Exception:
            return []

    def get_trust(self, text):
        # ë”ë¯¸ ì ìˆ˜: ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ë‚˜ ë¡œì§ìœ¼ë¡œ ê³„ì‚°
        return 85.0 

    def analyze_single(self, text, max_sent=3, keyword_num=5, max_input_sents=30):
        """
        ë‹¨ì¼ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ìš”ì•½, í‚¤ì›Œë“œ, ì„ë² ë”©ì„ ëª¨ë‘ ë°˜í™˜
        """
        clean_text = self._preprocess_text(text)
        if not clean_text:
            return "", [], np.zeros(768), np.zeros(768), np.zeros(768), 0.0

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(clean_text, top_n=keyword_num)
        
        # 2. ë³¸ë¬¸ ì„ë² ë”©
        content_emb = self.get_embedding_batch([clean_text], max_length=512)[0]
        
        # 3. í‚¤ì›Œë“œ ì„ë² ë”©
        keyword_text = " ".join(keywords) if keywords else ""
        keyword_emb = self.get_embedding_batch([keyword_text], max_length=64)[0] if keyword_text else np.zeros(768)
        
        # 4. ì‹ ë¢°ë„
        trust_score = self.get_trust(clean_text)

        # 5. ìš”ì•½ (ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ë°©ì‹)
        sents = [s.strip() for s in re.split(r'(?<=[.!?])\s+', clean_text) if len(s.strip()) > 20]
        sents_to_process = sents[:max_input_sents] if max_input_sents else sents

        if len(sents_to_process) <= max_sent:
            summary = clean_text
        else:
            sent_embs = self.get_embedding_batch(sents_to_process, max_length=128, batch_size=16)
            sim_matrix = cosine_similarity(sent_embs, sent_embs)
            scores = sim_matrix.sum(axis=1)
            top_indices = np.argsort(scores)[::-1][:max_sent]
            selected_idx = sorted(top_indices)
            summary = ' '.join([sents_to_process[i] for i in selected_idx])

        # 6. ìš”ì•½ë¬¸ ì„ë² ë”©
        summary_emb = self.get_embedding_batch([summary], max_length=256)[0] if summary else np.zeros(768)

        # ë°˜í™˜ (ìˆœì„œ: ìš”ì•½, í‚¤ì›Œë“œ, ë³¸ë¬¸ì„ë² ë”©, í‚¤ì›Œë“œì„ë² ë”©, ìš”ì•½ì„ë² ë”©, ì ìˆ˜)
        return summary, keywords, content_emb, keyword_emb, summary_emb, trust_score

# --- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ (ì´ íŒŒì¼ë§Œ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œ ë™ì‘) ---
if __name__ == "__main__":
    print("ğŸš€ NLP Engine Test Mode")
    analyzer = FastKoBertSummarizer()
    test_text = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì…ë‹ˆë‹¤. AIê°€ ì˜ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
    summ, kws, c_emb, k_emb, s_emb, score = analyzer.analyze_single(test_text)
    
    print(f"Summary: {summ}")
    print(f"Keywords: {kws}")
    print(f"Trust Score: {score}")
    print(f"Content Emb Shape: {c_emb.shape}")
